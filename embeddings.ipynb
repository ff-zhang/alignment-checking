{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T17:03:30.073478Z",
     "start_time": "2024-03-28T17:03:16.033589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-28 13:03:16--  http://nlp.stanford.edu/data/glove.6B.zip\r\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\r\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\r\n",
      "HTTP request sent, awaiting response... 302 Found\r\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\r\n",
      "--2024-03-28 13:03:16--  https://nlp.stanford.edu/data/glove.6B.zip\r\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\r\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\r\n",
      "--2024-03-28 13:03:16--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\r\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\r\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 862182613 (822M) [application/zip]\r\n",
      "Saving to: ‘glove.6B.zip’\r\n",
      "\r\n",
      "glove.6B.zip          8%[>                   ]  72.89M  5.12MB/s    eta 2m 9s  ^C\r\n",
      "Archive:  glove.6B.zip\r\n",
      "  End-of-central-directory signature not found.  Either this file is not\r\n",
      "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\r\n",
      "  latter case the central directory and zipfile comment will be found on\r\n",
      "  the last disk(s) of this archive.\r\n",
      "unzip:  cannot find zipfile directory in one of glove.6B.zip or\r\n",
      "        glove.6B.zip.zip, and cannot find glove.6B.zip.ZIP, period.\r\n"
     ]
    }
   ],
   "source": [
    "# run to download glove embeddings, total 822MB\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove.6B.zip -d glove\n",
    "!rm glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T17:03:34.185288Z",
     "start_time": "2024-03-28T17:03:30.075500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (1.26.4)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.11/site-packages (1.4.1.post1)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (1.12.0)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (1.3.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn) (3.2.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyenchant in /opt/homebrew/lib/python3.11/site-packages (3.2.2)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade numpy\n",
    "%pip install --upgrade scikit-learn\n",
    "%pip install --upgrade pyenchant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Glove Embeddings\n",
    "\n",
    "website available [here](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "- using embeddings trained on Wikipedia 2014 and Gigaword 5 data\n",
    "    - 6B tokens, 400K vocab, uncased, up to 300d vectors\n",
    "    - embeddings come in 50, 100, 200, and 300 dimensions\n",
    "- starting with 50d embeddings for speed\n",
    "    - can run with larger embeddings to see difference in algorithm performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T17:03:43.004536Z",
     "start_time": "2024-03-28T17:03:34.186794Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "import enchant\n",
    "\n",
    "# read the 50d embeddings and store in dictionary\n",
    "glove_50d = {}\n",
    "\n",
    "with open('glove/glove.6B.50d.txt') as f:\n",
    "    for line in f: # each line has a word followed by embedding vector (in a list)\n",
    "        values = line.split()\n",
    "        word = values[0] \n",
    "        \n",
    "        # filters out words which are solely numbers and/or punctuation\n",
    "        pattern = re.compile(\"[\\d{}]+$\".format(re.escape(string.punctuation)))\n",
    "        \n",
    "        # dictionaries to filter out any leftover word fragments\n",
    "        en_us = enchant.Dict(\"en_US\")\n",
    "        en_ca = enchant.Dict(\"en_CA\")\n",
    "        en_gb = enchant.Dict(\"en_GB\")\n",
    "        \n",
    "        if pattern.match(word) or not (en_us.check(word) and en_ca.check(word) and en_gb.check(word)):\n",
    "            continue\n",
    "\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        glove_50d[word] = vector\n",
    "\n",
    "# reverse the mapping\n",
    "glove_50d_inv = {tuple(v): k for k, v in glove_50d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T17:03:43.009052Z",
     "start_time": "2024-03-28T17:03:43.005967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n"
     ]
    }
   ],
   "source": [
    "# print the first word and its embedding\n",
    "w = list(glove_50d.keys())[0]\n",
    "print(w, glove_50d[w])"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['the',\n 'of',\n 'to',\n 'and',\n 'in',\n 'a',\n 'for',\n 'that',\n 'on',\n 'is',\n 'was',\n 'said',\n 'with',\n 'he',\n 'as',\n 'it',\n 'by',\n 'at',\n 'from',\n 'his',\n 'an',\n 'be',\n 'has',\n 'are',\n 'have',\n 'but',\n 'were',\n 'not',\n 'this',\n 'who',\n 'they',\n 'had',\n 'i',\n 'which',\n 'will',\n 'their',\n 'or',\n 'its',\n 'one',\n 'after',\n 'new',\n 'been',\n 'also',\n 'we',\n 'would',\n 'two',\n 'more',\n 'first',\n 'about',\n 'up',\n 'when',\n 'year',\n 'there',\n 'all',\n 'out',\n 'she',\n 'other',\n 'people',\n 'her',\n 'percent',\n 'than',\n 'over',\n 'into',\n 'last',\n 'some',\n 'government',\n 'time',\n 'you',\n 'years',\n 'if',\n 'no',\n 'world',\n 'can',\n 'three',\n 'do',\n 'president',\n 'only',\n 'state',\n 'million',\n 'could',\n 'us',\n 'most',\n 'against',\n 'so',\n 'them',\n 'what',\n 'him',\n 'united',\n 'during',\n 'before',\n 'may',\n 'since',\n 'many',\n 'while',\n 'where',\n 'states',\n 'because',\n 'now',\n 'city',\n 'made']"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(glove_50d_inv.values())[:100]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T17:03:43.014581Z",
     "start_time": "2024-03-28T17:03:43.009702Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T17:03:43.023674Z",
     "start_time": "2024-03-28T17:03:43.015462Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duckbill [ 0.33869   0.065012  0.24077   0.14919  -0.38003   0.34312   0.16798\n",
      " -0.14482   0.48706   0.3618   -0.093059  0.92098   1.0553    1.3386\n",
      " -0.35113  -0.12898  -0.44912   0.37596   0.27206   0.38409  -0.75018\n",
      " -0.87106   0.43528  -0.26207   0.40393   0.45237  -0.49666   0.53467\n",
      "  0.13618   0.12431  -1.1483   -1.1553    0.061     1.3135    0.065702\n",
      "  0.04004   0.25028  -0.45227   0.11692  -0.15344  -0.20095  -0.38547\n",
      " -0.41741   0.81822   0.22982  -0.44687   0.035491 -0.82128   0.26936\n",
      " -0.24443 ]\n"
     ]
    }
   ],
   "source": [
    "# create a random subset of 10000 words\n",
    "import random\n",
    "random.seed(0) # for reproducability\n",
    "glove_50d_subset = {k: glove_50d[k] for k in random.sample(list(glove_50d.keys()), 10000)}\n",
    "\n",
    "w = list(glove_50d_subset.keys())[0]\n",
    "print(w, glove_50d_subset[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T17:05:59.158503Z",
     "start_time": "2024-03-28T17:05:59.153897Z"
    }
   },
   "outputs": [],
   "source": [
    "# from https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py\n",
    "from time import time\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def bench_algorithm(algorithm, name, data, labels):\n",
    "    \"\"\"\n",
    "        - data passed in as ndarray of shape (n_samples, n_features)\n",
    "        - labels passed in as ndarray of shape (n_samples,)\n",
    "    \"\"\"\n",
    "    # metrics that require true labels and estimated labels\n",
    "    # https://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation\n",
    "    clustering_metrics = [\n",
    "        metrics.homogeneity_score,\n",
    "        metrics.completeness_score,\n",
    "        metrics.v_measure_score,\n",
    "        metrics.adjusted_rand_score,\n",
    "        metrics.adjusted_mutual_info_score,\n",
    "    ]\n",
    "\n",
    "    if hasattr(algorithm, 'labels'):\n",
    "        t0 = time()\n",
    "        estimator = make_pipeline(StandardScaler(), kmeans).fit(data) # scale data then run kmeans\n",
    "        fit_time = time() - t0\n",
    "    \n",
    "        results = [name, fit_time, estimator[-1].inertia_]\n",
    "        results += [m(labels, estimator[-1].labels_) for m in clustering_metrics]\n",
    "\n",
    "        # silhouette score for clusters\n",
    "        silhouette_score = metrics.silhouette_score(data, estimator[-1].labels_, metric=\"euclidean\", sample_size=300)\n",
    "        results += [silhouette_score]\n",
    "        \n",
    "    else:\n",
    "        t0 = time()\n",
    "        y = vae_gmm.fit_predict(glove_50d_subset_data)\n",
    "        fit_time = time() - t0\n",
    "        \n",
    "        results = [name, fit_time, 0.]\n",
    "        results += [m(labels, y) for m in clustering_metrics]\n",
    "\n",
    "        # silhouette score for clusters\n",
    "        silhouette_score = metrics.silhouette_score(data, y, metric=\"euclidean\", sample_size=300)\n",
    "        results += [silhouette_score]\n",
    "    \n",
    "    # print results\n",
    "    formatter_result = (\"{:9s}\\t{:.3f}s\\t{:.0f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\\t{:.3f}\")\n",
    "    print(formatter_result.format(*results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-28T17:08:32.388165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K means clustering on glove 50d embeddings\n",
      "__________________________________________________________________________________\n",
      "init\t\ttime\tinertia\thomo\tcompl\tv-meas\tARI\tAMI\tsilhouette\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "\n",
    "# arrange glove data into ndarray\n",
    "glove_50d_subset_data = np.array(list(glove_50d_subset.values()))\n",
    "glove_50d_subset_labels = np.array(list(glove_50d_subset.keys()))\n",
    "\n",
    "print('K means clustering on glove 50d embeddings')\n",
    "print(82 * '_')\n",
    "print('init\\t\\ttime\\tinertia\\thomo\\tcompl\\tv-meas\\tARI\\tAMI\\tsilhouette')\n",
    "\n",
    "kmeans = KMeans(init=\"k-means++\", n_clusters=1000, n_init=4, random_state=0)\n",
    "bench_algorithm(algorithm=kmeans, name=\"k-means++\", data=glove_50d_subset_data, labels=glove_50d_subset_labels)\n",
    "\n",
    "kmeans = KMeans(init=\"random\", n_clusters=1000, n_init=4, random_state=0)\n",
    "bench_algorithm(algorithm=kmeans, name=\"random\", data=glove_50d_subset_data, labels=glove_50d_subset_labels)\n",
    "\n",
    "vae_gmm = BayesianGaussianMixture(n_components=1000)\n",
    "bench_algorithm(algorithm=vae_gmm, name=\"vae gmm\", data=glove_50d_subset_data, labels=glove_50d_subset_labels)\n",
    "\n",
    "print(82 * '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# visualize the data in 2d\n",
    "pca = PCA(n_components=2)\n",
    "glove_50d_subset_data_pca = pca.fit_transform(glove_50d_subset_data)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(glove_50d_subset_data_pca[:, 0], glove_50d_subset_data_pca[:, 1], s=10, alpha=0.5, edgecolors='black')\n",
    "plt.title(\"PCA of glove 50d embeddings\")\n",
    "plt.xlabel(\"1st eigenvector\")\n",
    "plt.ylabel(\"2nd eigenvector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# run kmeans on the PCA data\n",
    "print('K means clustering on PCA of glove 50d embeddings')\n",
    "print(82 * '_')\n",
    "print('init\\t\\ttime\\tinertia\\thomo\\tcompl\\tv-meas\\tARI\\tAMI\\tsilhouette')\n",
    "\n",
    "kmeans = KMeans(init=\"k-means++\", n_clusters=1000, n_init=4, random_state=0)\n",
    "bench_algorithm(algorithm=kmeans, name=\"k-means++\", data=glove_50d_subset_data_pca, labels=glove_50d_subset_labels)\n",
    "\n",
    "kmeans = KMeans(init=\"random\", n_clusters=1000, n_init=4, random_state=0)\n",
    "bench_algorithm(algorithm=kmeans, name=\"random\", data=glove_50d_subset_data_pca, labels=glove_50d_subset_labels)\n",
    "\n",
    "vae_gmm = BayesianGaussianMixture(n_components=1000)\n",
    "bench_algorithm(algorithm=vae_gmm, name=\"vae gmm\", data=glove_50d_subset_data, labels=glove_50d_subset_labels)\n",
    "\n",
    "print(82 * '_')\n",
    "\n",
    "# VISUALIZATION NOT WORKING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Random Clusters for Alignment Checking\n",
    "- For the alignment checking algorithm, want to find random clusters of words in the embedding space\n",
    "    - We can find clusters of 100 words by randomly selecting a point in the embedding space and finding the closest 100 points\n",
    "- Large size of the vocab leads to obscure words selected for clusters\n",
    "    - Can also define \"meaningful\" clusters by choosing certain words to be in the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# select 100 random words\n",
    "random.seed(0)\n",
    "random_words = random.sample(list(glove_50d.keys()), 100)\n",
    "random_words_embeddings = np.array([glove_50d[w] for w in random_words])\n",
    "\n",
    "glove_50d_data = np.array(list(glove_50d.values()))\n",
    "glove_50d_labels = np.array(list(glove_50d.keys()))\n",
    "\n",
    "# for each word, find the 99 nearest neighbours by euclidean distance\n",
    "from sklearn.metrics import pairwise_distances\n",
    "distances = pairwise_distances(random_words_embeddings, glove_50d_data, metric='euclidean')\n",
    "nearest_neighbours = np.argsort(distances, axis=1)[:, 1:100]\n",
    "\n",
    "# print the first 5 random words and their 5 nearest neighbours\n",
    "for i, w in enumerate(random_words):\n",
    "    print(w, [glove_50d_inv[tuple(glove_50d_data[j])] for j in nearest_neighbours[i]][:5])\n",
    "    if i == 5 : break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".csc413",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
